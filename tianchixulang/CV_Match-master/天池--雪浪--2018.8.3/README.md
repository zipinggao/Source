# CV_Match

比赛地址：https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11165261.5678.1.e8bb419dOlturj&raceId=231666

初赛A榜54，B榜52，成绩A榜0.911，B榜0.912。

这里就写写我们的比赛想法和赛后的一些反思，感谢队友zp比赛时候的支持。

# 环境：
OS：linux Ubuntu 16.04

Cuda：9.0

Cudnn：7.0

框架：keras 2.2.0   github地址：https://github.com/keras-team/keras

工具包：imgaug（数据增强）   github地址：https://github.com/aleju/imgaug


# 比赛思路：
比赛的目的是让我们分析布匹是否有瑕疵，给出的数据包括了图片，图片的相应类别，以及图片中瑕疵（假如有）的bounding box。我们将思路分成了两大块，第一大块是分类，第二大块是识别。

## 分类
提到分类很容易想到的就是把图片扔进imagenet预训练的各种分类模型。由于给的图片数据的分辨率高达2560x1920，我们用脚本画出所给瑕疵的bbox发现，很多bbox都很小，如果我们简简单单的将图片resize到224x224，我们就很有可能丢失这些瑕疵的特征（这里也是因为看了很多目标检测的论文，增大分辨率会提升小物体的检测能力给了我一些intuition）。为了最大程度的保持分辨率，我们得出了以下两种方法：

### 方法1：
对图片进行切割，即有瑕疵的训练样本围绕瑕疵的bbox进行切割，如果bbox小于我们要切割出来的分辨率，就切割出包裹bbox的图，大于的话就沿着bbox中线从左到右从上到下切割。正常样本就长宽都每隔112像素切割一次。我们先按224x224进行了切分（分辨率0损失）。这样我们就得到了几万张有瑕疵的图和30万张正常的图，我们对有瑕疵的图进行随机翻转和变色的数据增强到10万张，无瑕疵的进行随机抽取到10万张，这样保证正负样本平衡。训练阶段就是把图片直接扔进Densenet201中。测试阶段则是将测试图按224x224每隔112切割，得到的每张图片都经过网络预测，取网络预测图片有瑕疵概率的最大值（这几万张图片中瑕疵概率的最值）。依靠这种方法我们得到了0.7874的成绩（此处吐血）。

我们认为第一次的提交成绩如此低的原因在于网络想要判断一张图片没有瑕疵（或者瑕疵几率较低），需要将切分出的几万张图片都判断为没有瑕疵或者至少是低瑕疵几率，一旦其中有一张被判断失误，那这张图片的预测值就会出现非常大的改变。即使模型错误率是0.01%，10000张图判断错一张的小失误都会使我们的结果变得很差。
为了减少切分出的图片张数，我们决定放弃一些分辨率。将图片切成960x960且不进行有交叉的切分了（训练集也切成960x960），这样一张测试图只会被切成6张。我们这次换用了inception-resnetv2网络，resize到448x448（nas实在是难以训练），做数据增强到了正常和有瑕疵都为1万张，其实可以更多的，但是我们只是想先看一下方法的可行性所以没有增强到很多张。效果和预期的方向相同，线上有0.8720。

### 方法2：
上面的成绩依旧无法令我们安稳的进入复赛，所以我们只好另寻他法。我们开始觉得切分或许是一个错误的决定，因为不管切分为多少张，将多张同时预测正确的难度往往是比将一张预测正确的难度要高的。因为时间很有限（导师项目很紧，没啥时间研究模型），我们放弃了继续优化方法1。我们决定直接将图片resize到一个比较大的分辨率，扔到全卷积的网络中，最后的特征图无论多大都直接做avgpooling。这里的resize比较特殊（全靠队友zp carry），因为我们发现大的布匹图片默认resize后会出现条状波纹，使用cv2.resize(img, (800, 600), interpolation=cv2.INTER_AREA)可以解决这个问题。我们选择了600x800的大小进行了resize和数据增强，直接通过inception_resnet_v2后最后一层的特征图大小高达17x23(我认为有些大）。batch为8，正负样本各5000张跑完后的结果在线上达到了0.911，也是我们的最好成绩。

虽然0.911的成绩复赛可以说是比较稳了，但我觉得这肯定不是模型的终点，我认为最后一层较大的特征图直接做avgpool得到的特征质量不如再加几层卷积。我也进行了尝试，增加了一个类似inception的卷积层试验了一下（stride=2），结果从0.911滑落到了0.897。可能是由于增加了卷积层所以增加了参数导致了过拟合。


### 以下是图像分类的相关脚本：
classifier/img_aug.ipynb:图像数据增强的脚本，使用了imgaug包，对图像进行了随机的上下翻转，左右翻转和明暗的轻微变化。

classifier/keras model.ipynb:模型训练的脚本

classifier/test.ipynb:生成测试csv的脚本


## 目标检测
我们想到的另一个模型方向是建立一个目标检测模型。我们一开始没有使用目标检测模型的原因在于我们认为目标检测模型更复杂，会需要更多的数据量，而初赛的数据量尤其是瑕疵样本的数据量太少了。另一个原因是现有的目标检测模型：yolo，ssd，faster-rcnn对于小目标的检测能力都很一般，对于小目标比较多的数据集来说预期效果就会比较差。但本着尝试积累经验的态度，我们还是搭建了一个caffe2环境跑了一遍faster-rcnn。

我们使用了Facebook的Detectron实现。   github地址：https://github.com/facebookresearch/Detectron

使用了其中的以FPN为backbone的Faster-Rcnn：配置文件为 e2e_faster_rcnn_X-101-32x8d-FPN_1x.yaml

数据方面因为类别极不均衡（尤其好几类只有一张图片）我们对这些图片进行了过采样（亮度变换，bbox不变），小于15张的一律增强到15张，之后使用上下翻转，左右翻转和旋转180度，此处照着keras-retinanet（https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/utils/transform.py） 中的数据增强方式仿写，图片数量增强到原来的3倍，并伴随bbox的位置变换。这样我们就得到了一共4236张有bbox标记的有瑕疵图片，我们自己写了个脚本先将voc格式的xml转化为了txt，之后又转化为了coco格式（Detectron需要的格式，当然这一系列操作有点脑残），分辨率resize到960x1280（未使用上述的cv2的resize方法，可能是一个问题）batch为1，跑了360000个step，学习率在240000和320000的时候会衰减到原来的1/10，初始学习率0.0025，使用两个Titan Xp训练了3天。预测的时候取几率最大的bbox的prob为那张图片有瑕疵的prob。线上结果为0.8984。



# 反思
其实每次比赛过后看到排行榜前列的大佬都很羡慕，其实大家的模型应该都跑的差不多，但是别人线上就可以高你5个点，但苦于身边能交流这方面比赛的人实在是太少了。。。导致比赛完感觉自己就搭了个环境跑了个模型，提高也说不上，也没有积攒到一些技巧和trick，真心希望能多些人交流这方面的东西。

回到正题，这次比赛首先应该反思的是数据增强部分，对于此次比赛而言其实一张图可以左右翻转，旋转90度+左右，旋转180度+左右，旋转270度+左右这样能1张变8张，而我只增强出了其中的4张，增强的时候没有考虑清楚，这样就平白少了一倍的数据量。（怕旋转或者剪切后将瑕疵部位去除，所有没有做，应该通过计算旋转或者剪切后的bbox来看瑕疵是否在图片内来进行数据增强后的筛选的）

其次，比赛中先入为主的觉得应该使用高分辨率+保持比例来确保瑕疵特征出现，却没有尝试resize到正方形或者resize到224或者299这种正常的size，应该花时间进行尝试的。

再次，目标检测模型的学习率设置和数据增强应该还是有些问题，论坛上有人使用one-stage的retinanet线上达到了0.92，速度更慢，但准确率更高的two-stage frcn不应该只有0.89。

能想到的反思大约就是这些，应该还有其它各种trick或者模型上的改进，欢迎交流。










