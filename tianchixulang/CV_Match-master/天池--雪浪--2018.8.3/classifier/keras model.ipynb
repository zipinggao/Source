{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten,BatchNormalization,Activation,AveragePooling2D,Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model  \n",
    "from keras.layers import GlobalAveragePooling2D,Flatten\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_resnet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inception_resnet_v2.InceptionResNetV2(weights='imagenet', include_top=False,input_shape=(600,800,3))\n",
    "#densenet_model = load_model(\"high_auc_0.8924897119341563checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"nasnet_large_no_top.h5\", by_name=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              kernel_size,\n",
    "              strides=1,\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              use_bias=False,\n",
    "              name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: filters in `Conv2D`.\n",
    "        kernel_size: kernel size as in `Conv2D`.\n",
    "        strides: strides in `Conv2D`.\n",
    "        padding: padding mode in `Conv2D`.\n",
    "        activation: activation in `Conv2D`.\n",
    "        use_bias: whether to use a bias in `Conv2D`.\n",
    "        name: name of the ops; will become `name + '_ac'` for the activation\n",
    "            and `name + '_bn'` for the batch norm layer.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "    \"\"\"\n",
    "    x = Conv2D(filters,\n",
    "              kernel_size,\n",
    "              strides=strides,\n",
    "              padding=padding,\n",
    "              use_bias=use_bias,\n",
    "              name=name)(x)\n",
    "    if not use_bias:\n",
    "        bn_axis = 1 if backend.image_data_format() == 'channels_first' else 3\n",
    "        bn_name = None if name is None else name + '_bn'\n",
    "        x = BatchNormalization(axis=bn_axis,\n",
    "                                      scale=False,\n",
    "                                      name=bn_name)(x)\n",
    "    if activation is not None:\n",
    "        ac_name = None if name is None else name + '_ac'\n",
    "        x = Activation(activation, name=ac_name)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as backend\n",
    "\n",
    "#尝试添加了一层类似inception的卷积层，降低featuremap大小，通道数不变，效果不太好。\n",
    "\n",
    "# x = model.layers[-4].output\n",
    "# #17*23*2080\n",
    "# x1 = conv2d_bn(x,256,1)\n",
    "# x1 = conv2d_bn(x1,384,3,padding='valid',strides=2)\n",
    "# x2 = conv2d_bn(x,256,1)\n",
    "# x2 = conv2d_bn(x2,384,3,padding='valid',strides=2)\n",
    "# x2 = conv2d_bn(x2,384,1)\n",
    "# x3 = conv2d_bn(x,384,1)\n",
    "# x3 = AveragePooling2D(pool_size=(3,3),padding='valid',strides=2)(x3)\n",
    "# x4 = conv2d_bn(x,384,1)\n",
    "# x4 = MaxPooling2D(pool_size=(3,3),padding='valid',strides=2)(x4)\n",
    "\n",
    "# x = Concatenate(axis=3, name='mixed_lbc')([x1,x2,x3,x4])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "predictions = Dense(2,activation='softmax',name='predictions')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model = Model(inputs = model.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001)\n",
    "#densenet_model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "print(\"正常\",len(os.listdir(\"../disk1/train_aug_600_800/normal/\")))\n",
    "print(\"异常\",len(os.listdir(\"../disk1/train_aug_600_800/bad/\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正常的\n",
    "#使正负样本平衡\n",
    "# list_positive = os.listdir(\"xuelang/crop_data/crop_positive\")\n",
    "# random.shuffle(list_positive)\n",
    "# list_positive_22758 = list_positive[:22758]\n",
    "# import shutil\n",
    "# if \"crop_positive_22758\" not in os.listdir(\"xuelang/crop_data\"):\n",
    "#     os.mkdir(\"xuelang/crop_data/crop_positive_22758\")\n",
    "# for i in list_positive_22758:\n",
    "#     shutil.copy(\"xuelang/crop_data/crop_positive/\"+i,\"xuelang/crop_data/crop_positive_22758/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rescale=1./255)\n",
    "train_generator = image_generator.flow_from_directory(directory='../disk1/train_aug_600_800/',\n",
    "                                                     target_size=(600,800),\n",
    "                                                     batch_size=8,\n",
    "                                                     class_mode='categorical')\n",
    "val_generator = image_generator.flow_from_directory(directory='xuelang/crop_data/val/',\n",
    "                                                     target_size=(600,800),\n",
    "                                                     batch_size=8,\n",
    "                                                     class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as backend\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback): \n",
    "    def __init__(self, model, path):\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        self.best_loss = np.inf\n",
    "        self.best_auc = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "#         val_loss = logs['val_loss']\n",
    "#         if val_loss < self.best_loss: \n",
    "#             print(\"\\nValidation loss decreased from {} to {}, saving model\".format(self.best_loss, val_loss))\n",
    "#             self.model.save(self.path)\n",
    "#             self.best_loss = val_loss\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        y_val_predict_all = []\n",
    "        y_val = []\n",
    "        #读取验证集normal的一张图像\n",
    "        for i in os.listdir(\"xuelang/crop_data/val/normal\"):\n",
    "            img = cv2.imread(\"xuelang/crop_data/val/normal/\" + i)\n",
    "            #转换为rgb\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#             batch_x = np.zeros((289,) + (224,224,3),backend.floatx())\n",
    "            img = cv2.resize(img, (800, 600), interpolation=cv2.INTER_AREA)\n",
    "            batch_x = np.zeros((1,600,800,3),backend.floatx())\n",
    "            batch_x[0] = img\n",
    "#             batch_x = np.zeros((6,299,299,3),backend.floatx())\n",
    "#             x = 0\n",
    "#             for i in range(3):\n",
    "#                 y = 0\n",
    "#                 for j in range(2):\n",
    "#                     batch_x[i * 2 + j] = cv2.resize(img[y:y + 960, x:x + 960], (299, 299), interpolation=cv2.INTER_CUBIC)\n",
    "#                     y = y + 960\n",
    "#                 x = x + 800\n",
    "            y_val_predict = self.model.predict(batch_x/255.,verbose=0,batch_size=8)\n",
    "            #取其中可能性最大的为该图片的有问题与否可能性\n",
    "            #print(y_val_predict)\n",
    "            y_val_predict = np.max(y_val_predict,axis=0)[0]\n",
    "            #print(y_val_predict)\n",
    "            y_val_predict_all.append(y_val_predict)\n",
    "            #normal 可能性应该为0\n",
    "            y_val.append(0)\n",
    "        print(\"正常的预测完成\")\n",
    "        \n",
    "        #读取验证集bad的一张图像\n",
    "        for i in os.listdir(\"xuelang/crop_data/val/bad\"):\n",
    "            img = cv2.imread(\"xuelang/crop_data/val/bad/\" + i)\n",
    "            #转换为rgb\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (800, 600), interpolation=cv2.INTER_AREA)\n",
    "            batch_x = np.zeros((1,600,800,3),backend.floatx())\n",
    "            batch_x[0] = img\n",
    "#             x = 0\n",
    "#             for i in range(3):\n",
    "#                 y = 0\n",
    "#                 for j in range(2):\n",
    "#                     batch_x[i * 2 + j] = cv2.resize(img[y:y + 960, x:x + 960], (299, 299), interpolation=cv2.INTER_CUBIC)\n",
    "#                     y = y + 960\n",
    "#                 x = x + 800\n",
    "#             x = 0\n",
    "            \n",
    "            y_val_predict = self.model.predict(batch_x/255.,verbose=0,batch_size=8)\n",
    "        \n",
    "            #取其中可能性最大的为该图片的有问题与否可能性\n",
    "            y_val_predict = np.max(y_val_predict,axis=0)[0]\n",
    "            y_val_predict_all.append(y_val_predict)\n",
    "            #bad 可能性应该为1\n",
    "            y_val.append(1)\n",
    "        print(\"瑕疵的预测完成\")\n",
    "        \n",
    "        print(\"验证集验证完成\")\n",
    "        \n",
    "        end = time.time()\n",
    "\n",
    "        print(\"用时:\",end - start)\n",
    "        \n",
    "        \n",
    "        from sklearn import metrics\n",
    "        val_auc = metrics.roc_auc_score(y_val,y_val_predict_all)\n",
    "        print('val_auc:',val_auc)\n",
    "        if val_auc > self.best_auc or val_auc > 0.9: \n",
    "            print(\"\\nValidation auc increased from {} to {}, saving model\".format(self.best_auc, val_auc))\n",
    "            self.model.save('high_auc_'+str(val_auc)+self.path)\n",
    "            self.best_auc = val_auc\n",
    "            \n",
    "            \n",
    "        abc = np.array(y_val_predict_all)\n",
    "        abc[abc>=0.5] = 1\n",
    "        abc[abc<0.5] = 0\n",
    "        \n",
    "        print(\"accuracy:\",np.sum(abc == np.array(y_val))/len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import SGD\n",
    "#model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "# Replicates `model` on 8 GPUs.\n",
    "# This assumes that your machine has 8 available GPUs.\n",
    "parallel_model = multi_gpu_model(densenet_model, gpus=2)\n",
    "parallel_model.compile(loss='categorical_crossentropy',optimizer=adam,metrics = ['accuracy'])\n",
    "#parallel_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = parallel_model.fit_generator(train_generator,validation_data=val_generator,verbose=1,epochs=100,callbacks=[CustomModelCheckpoint(densenet_model,'68checkpoint.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
